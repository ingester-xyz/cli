{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ingester CLI \ud83d\udeb0","text":"<p>Welcome to Ingester, a CLI tool to easily ingest AWS S3 objects into Walrus storage.</p> <p></p> <p>This site contains everything you need to understand, install, and operate the tool.</p> <p>\ud83d\udc49 Jump to:</p> <ul> <li>What is Ingester?</li> <li>Why Ingester?</li> <li>Technical Guide</li> <li>Examples</li> </ul>"},{"location":"examples/examples/","title":"Ingester CLI examples \ud83d\udeb0","text":""},{"location":"examples/examples/#examples-are-available-here","title":"Examples are available here.","text":"<p>These examples provides a set of scripts for ingesting files into Walrus, either from a local file or from an S3 bucket.</p> <ul> <li><code>file-ingestion.sh</code>: Ingests a local file into Walrus.</li> <li><code>s3-aws-ingestion.sh</code>: Ingests files from an S3 bucket into Walrus.</li> </ul> <p>Make sure to configure your environment variables (<code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_REGION</code>, <code>AWS_S3_BUCKET</code>) correctly before running the scripts.</p> <p>The scripts use the <code>ingester</code> Go CLI to perform the ingestion process \ud83d\udeb0.</p>"},{"location":"examples/examples/#folder-structure","title":"Folder Structure","text":"<p>The <code>examples/</code> folder contains the following scripts:</p> <ul> <li><code>file-ingestion.sh</code>: Ingests a local file into Walrus.</li> <li><code>s3-aws-ingestion.sh</code>: Ingests files from an S3 bucket into Walrus.</li> <li><code>env</code>: Contains the AWS environment variables required to interact with the S3 bucket.</li> </ul>"},{"location":"examples/examples/#prerequisites","title":"Prerequisites","text":""},{"location":"examples/examples/#1-go-environment","title":"1. Go Environment","text":"<p>Ensure Go is installed and configured on your machine. To check if Go is installed, run:</p> <pre><code>go version\n</code></pre> <p>If Go is not installed, follow the Go installation guide to install Go.</p>"},{"location":"examples/examples/#3-ingest-cli-binary","title":"3. Ingest CLI Binary","text":"<p>The <code>ingester</code> binary must be built using the Go source code before running the ingestion scripts. The <code>file-ingestion.sh</code> and <code>s3-aws-ingestion.sh</code> scripts will automatically build the binary.</p>"},{"location":"examples/examples/#setup-and-configuration","title":"Setup and Configuration","text":""},{"location":"examples/examples/#1-set-environment-variables","title":"1. Set Environment Variables","text":"<p>In the <code>env</code> file located in the <code>examples/</code> folder, you must configure your AWS credentials and the S3 bucket name.</p> <p>Example of the <code>env</code> file:</p> <pre><code>#!/bin/bash\n\n# AWS env variables to provide S3 bucket read+list access\nexport AWS_ACCESS_KEY_ID=\"&lt;your-aws-access-key-id&gt;\"\nexport AWS_SECRET_ACCESS_KEY=\"&lt;your-aws-secret-access-key&gt;\"\nexport AWS_REGION=\"&lt;your-aws-default-region&gt;\"\n\nexport AWS_S3_BUCKET=\"&lt;your-aws-s3-bucket-name-to-ingest-from&gt;\"\n</code></pre> <p>Replace the placeholders (<code>&lt;your-aws-access-key-id&gt;</code>, <code>&lt;your-aws-secret-access-key&gt;</code>, etc.) with your actual AWS credentials and the S3 bucket name.</p>"},{"location":"examples/examples/#running-the-scripts","title":"Running the Scripts","text":""},{"location":"examples/examples/#1-ingesting-a-local-file","title":"1. Ingesting a Local File","text":"<p>To ingest a local file (<code>./assets/sample.webp</code>) into Walrus, run the following script:</p> <pre><code>./examples/file-ingestion.sh\n</code></pre> <p>This script does the following:</p> <ol> <li>Sets the necessary environment variables.</li> <li>Checks if the local file exists (<code>./assets/sample.webp</code>).</li> <li>Builds the Go <code>ingester</code> binary.</li> <li>Executes the ingestion of the local file using the <code>ingester local file</code> command.</li> <li>Cleans up after the ingestion.</li> </ol>"},{"location":"examples/examples/#2-ingesting-files-from-an-s3-bucket","title":"2. Ingesting Files from an S3 Bucket","text":"<p>To ingest files from an S3 bucket into Walrus, run the following script:</p> <pre><code>./examples/s3-aws-ingestion.sh\n</code></pre> <p>This script performs the following:</p> <ol> <li>Sets the necessary environment variables for AWS access.</li> <li>Checks if the required AWS environment variables are set.</li> <li>Verifies that the AWS CLI is installed and configured.</li> <li>Builds the Go <code>ingester</code> binary.</li> <li>Executes the ingestion process using the <code>ingester s3</code> command with the configured S3 bucket and region.</li> </ol>"},{"location":"examples/examples/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any errors while running the scripts, check the following:</p> <ol> <li>AWS Environment Variables: Ensure the AWS environment variables are correctly set in the <code>env</code> file.</li> <li>File Path: For <code>file-ingestion.sh</code>, ensure the file path (<code>./assets/sample.webp</code>) exists and is accessible.</li> <li>Permission Issues: Make sure your AWS user has the necessary permissions to access the S3 bucket (<code>s3:ListBucket</code>, <code>s3:GetObject</code>).</li> </ol>"},{"location":"guide/commands/","title":"CLI Commands","text":""},{"location":"guide/commands/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Go (v1.18 or later) installed and on your <code>PATH</code>.</p> </li> <li> <p>AWS credentials (with S3 read access) available via environment variables:</p> </li> </ol> <pre><code>export AWS_ACCESS_KEY_ID=&lt;your-aws-access-key-id&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;your-aws-secret-access-key&gt;\nexport AWS_REGION=&lt;your-aws-default-region&gt;\n</code></pre> <ol> <li>Walrus endpoints (public gateway URLs) configured in your shell:</li> </ol> <pre><code># Single var for both read/write\nexport WALRUS_ENDPOINT=\"https://aggregator.walrus-testnet.walrus.space,https://publisher.walrus-testnet.walrus.space\"\n\n# Or separate for more control\nexport WALRUS_AGGREGATOR_URLS=\"https://aggregator.walrus-testnet.walrus.space\"\nexport WALRUS_PUBLISHER_URLS=\"https://publisher.walrus-testnet.walrus.space\"\n</code></pre>"},{"location":"guide/commands/#building","title":"Building","text":"<pre><code># Clone the repository\ngit clone git@github.com:ingester-xyz/cli.git\ncd cli\n\n# Download dependencies\ngo mod tidy\n\n# Build the binary\ngo build -o ingester .\n</code></pre> <p>This generates an executable named <code>ingester</code> in your project root.</p>"},{"location":"guide/commands/#commands-overview","title":"Commands Overview","text":"<p>All commands live under the <code>ingester</code> root command:</p> Command Description <code>s3</code> Ingests all objects from a bucket into Walrus and persists refs metadata <code>list</code> Lists all ingested S3 keys from a metadata blob <code>get</code> Retrieves a single file by its S3 key and writes to stdout or file <code>url</code> Get public URL for AWS S3 ingested data key in Walrus"},{"location":"guide/commands/#1-ingester-s3","title":"1. <code>ingester s3</code>","text":"<p>Downloads every object from an S3 bucket, uploads each to Walrus, then writes a single metadata blob containing the S3-to-BlobID map.</p> <pre><code>ingester s3 \\\n  --bucket my-bucket       \\\n  --region us-west-2\n# \u2192 Refs metadata stored as blob: QmSvz\u2026Yz123\n</code></pre> <ul> <li> <p>Flags:</p> </li> <li> <p><code>--bucket</code> (string, required)</p> </li> <li><code>--region</code> (string, required)</li> <li>Other flags (<code>--prefix</code>, <code>--tags</code>, etc.) are reserved for future use and currently ignored.</li> </ul>"},{"location":"guide/commands/#2-ingester-list","title":"2. <code>ingester list</code>","text":"<p>Prints all original S3 keys stored in the given metadata blob.</p> <pre><code>ingester list --meta-blob-id QmSvz\u2026Yz123\n</code></pre> <ul> <li> <p>Flags:</p> </li> <li> <p><code>--meta-blob-id</code> (string, required)</p> </li> </ul>"},{"location":"guide/commands/#3-ingester-get","title":"3. <code>ingester get</code>","text":"<p>Fetches one ingested file by its original S3 key and writes the raw bytes to stdout or a file.</p> <pre><code># To stdout:\ningester get \\\n  --meta-blob-id QmSvz\u2026Yz123 \\\n  --key path/to/file.txt\n\n# To a file:\ningester get \\\n  --meta-blob-id QmSvz\u2026Yz123 \\\n  --key images/photo.png \\\n</code></pre> <ul> <li> <p>Flags:</p> </li> <li> <p><code>--meta-blob-id</code> (string, required)</p> </li> <li><code>--key</code> (string, required)</li> </ul>"},{"location":"guide/commands/#example-workflow","title":"Example Workflow","text":"<ol> <li>Ingest an S3 bucket into Walrus:</li> </ol> <pre><code>export AWS_ACCESS_KEY=&lt;you-aws-access-key&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;you-aws-secret-key&gt;\nexport AWS_REGION=&lt;you-aws-region&gt;\nexport WALRUS_ENDPOINT=\"https://aggregator.walrus-testnet.walrus.space,https://publisher.walrus-testnet.walrus.space\"\n\n./ingester s3 --bucket my-test-bucket --region eu-west-1\n# \u2192 Refs metadata stored as blob: QmSvz\u2026Yz123\n</code></pre> <ol> <li>List all ingested keys:</li> </ol> <pre><code>./ingester list --meta-blob-id QmSvz\u2026Yz123\n</code></pre> <ol> <li>Retrieve one file:</li> </ol> <pre><code>./ingester get --meta-blob-id QmSvz\u2026Yz123 --key path/to/file.txt\n</code></pre>"},{"location":"intro/what-is-ingester/","title":"What is Ingester?","text":"<p>Ingester is a Go-based CLI tool designed to ingest data into Walrus seamless and predictable.</p> <p>Like opening the tap \ud83d\udeb0</p> <p>For this first iteration, it helps migrating existing data from AWS S3 into Walrus.</p> <p></p> <p>More integrations are ready to come (e.g. Arweave -&gt; Walrus).</p>"},{"location":"intro/what-is-ingester/#context","title":"Context","text":"<p>Today, Walrus stores data as scattered blobs, with no built-in structure, and ingesting data into it is a manual, fragmented process. As a result, every team has to build their own solution.</p> <p>Ingester solves these two critical pain points by:</p> <ul> <li> <p>Adding an automatic, schema-driven structure that organizes and relates blobs predictably.</p> </li> <li> <p>Providing ready-to-use integrations (like AWS S3) so teams can easily ingest data without extra development effort.</p> </li> </ul> <p>With Ingester, Walrus transforms from a raw storage system into a plug-and-play, developer-friendly platform. Therefore accelerating adoption, reducing barriers, and unlocking its full potential across real-world use cases.</p>"},{"location":"intro/why-ingester/","title":"Why use Ingester?","text":""},{"location":"intro/why-ingester/#the-problem","title":"The Problem","text":"<p>Walrus stores data as individual blobs, without inherent structure.</p> <p>Developers struggle to manage and relate these blobs meaningfully.</p> <p>Ingesting data into Walrus today is manual and fragmented \u2014 each team must build its own solution.</p> <p>This slows adoption and creates unnecessary technical barriers.</p>"},{"location":"intro/why-ingester/#the-solution","title":"The Solution","text":"<p>Provides an automatic, schema-driven structure over Walrus blobs.</p> <p>Offers ready-made integrations (AWS S3, and more) to simplify ingestion.</p> <p>Delivers a plug-and-play experience where developers can just use Walrus, without reinventing ingestion pipelines.</p> <p>Frees teams from worrying about data relationships or implementation details \u2014 it just works.</p>"},{"location":"intro/why-ingester/#value-proposition","title":"Value Proposition","text":"<p>Ingester simplifies how teams use Walrus by solving two critical problems:</p> <ul> <li> <p>It adds structure \u2014 turning scattered blobs into organized, connected data, so developers don\u2019t have to figure it out themselves.</p> </li> <li> <p>It removes friction \u2014 offering ready-to-use integrations that make ingesting data easy, saving teams from wasting time building custom solutions.</p> </li> </ul> <p>With Ingester, Walrus becomes a plug-and-play, developer-friendly storage platform, accelerating adoption across diverse industries and unlocking the full potential of decentralized storage.</p>"}]}